# Test configuration file for CI environments

app_name = "code-story"
version = "0.1.0"
environment = "development"

[neo4j]
# Use Docker service name and default port when running in Docker Compose
uri = "bolt://localhost:7687"
username = "neo4j"
password = "password"
database = "testdb"

[redis]
# Use Docker service name and default port when running in Docker Compose
uri = "redis://redis:6379/0"  # Use this for Docker Compose; fallback to localhost:6380 for local

[openai]
api_key = "sk-test-key-openai"
embedding_model = "text-embedding-3-small"
chat_model = "gpt-4o"
reasoning_model = "gpt-4o"
endpoint = "<your-endpoint>"

[azure_openai]
api_key = "sk-test-key-azure"
endpoint = "<your-endpoint>"
deployment_id = "gpt-4o"
embedding_model = "text-embedding-3-small"
chat_model = "gpt-4o"
reasoning_model = "gpt-4o"

[service]
host = "0.0.0.0"
port = 9000
workers = 4
log_level = "INFO"
enable_telemetry = true
worker_concurrency = 4

[ingestion]
config_path = "pipeline_config.yml"
chunk_size = 1024
chunk_overlap = 200
embedding_model = "text-embedding-3-small"
embedding_dimensions = 1536
max_retries = 3
retry_backoff_factor = 2.0
concurrency = 5

[plugins]
enabled = ["blarify", "filesystem", "summarizer", "docgrapher"]
plugin_directory = "plugins"

[telemetry]
metrics_port = 9090
metrics_endpoint = "/metrics"
trace_sample_rate = 1.0
log_format = "text"

[interface]
theme = "dark"
default_view = "graph"
graph_layout = "force"
max_nodes = 1000
max_edges = 5000
auto_refresh = true
refresh_interval = 30

[azure]
keyvault_name = ""
tenant_id = ""
client_id = ""
client_secret = ""